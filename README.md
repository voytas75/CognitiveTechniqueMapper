# Cognitive Technique Mapper

Cognitive Technique Mapper (CTM) is a modular CLI application that pairs problem descriptions with the most suitable cognitive or problem-solving techniques. It combines a structured SQLite knowledge base, optional Chroma vector search, and workflow-specific LLM reasoning orchestrated through `litellm`.

---

## üöÄ Features

- **Technique Knowledge Base:** SQLite schema for storing named techniques with descriptions, origins, categories, and core principles. Optional Chroma collection holds vector embeddings for semantic search.
- **Workflow-Oriented Reasoning:** `litellm` gateway routes requests to different models per workflow (detection, explanation, summarization, feedback).
- **Structured Recommendations:** `analyze` now parses LLM replies into technique, rationale, and concrete steps, and automatically generates an implementation plan.
- **Structured Explanations:** `explain` renders JSON-backed insights covering key factors, risks, and suggested follow-ups.
- **Scenario Simulation:** `simulate` explores how the recommended technique performs under different what-if paths and highlights cautions.
- **Candidate Comparison:** `compare` contrasts the shortlist, surfaces the strongest alternative, and offers decision guidance.
- **Preference-Aware Personalization:** Feedback ratings train a lightweight preference model that influences recommendations and prompts.
- **CLI Experience:** `describe`, `analyze`, `explain`, `settings`, `refresh`, and `feedback` commands guide users from problem input to actionable recommendations.
- **Config-Driven:** YAML files under `config/` define app metadata, database paths, model mappings, and providers.
- **Bootstrap & Persistence:** `TechniqueDataInitializer` seeds the database (and Chroma) from `data/techniques.json`, while CLI state persistence (`data/state.json`) allows multi-command sessions.
- **Interactive Technique Catalog:** `techniques` subcommands let you list/add/update/remove techniques with automatic dataset and embedding synchronization.
- **History & Preferences:** `history` commands reveal or clear session context, while `preferences` surfaces personalization signals and allows resets.
- **Bulk Catalog I/O:** `techniques export` and `techniques import` handle JSON backups and restores with optional append mode and embedding rebuilds.
- **Interactive Configuration:** `settings show`, `settings update-workflow`, and `settings update-provider` offer in-CLI editing with optional interactive prompts.
- **Dataset Refresh:** `refresh` rebuilds the SQLite dataset and (optionally) regenerates vector embeddings without manual file management.
- **Persistent Feedback:** Feedback entries are stored in SQLite, so summaries span sessions and retain historical context.

---

## üóÇÔ∏è Project Structure (excerpt)

```
.
‚îú‚îÄ‚îÄ config/                # settings.yaml, models.yaml, providers.yaml
‚îú‚îÄ‚îÄ data/                  # techniques.json, state.json, SQLite DB
‚îú‚îÄ‚îÄ embeddings/            # Chroma DB persistence
‚îú‚îÄ‚îÄ prompts/               # Prompt templates and registry
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli.py             # Typer CLI entry point
‚îÇ   ‚îú‚îÄ‚îÄ core/              # Orchestrator, LLM gateway, config loader, etc.
‚îÇ   ‚îú‚îÄ‚îÄ db/                # SQLite and Chroma clients
‚îÇ   ‚îú‚îÄ‚îÄ services/          # Technique selector, embeddings, plan generator‚Ä¶
‚îÇ   ‚îî‚îÄ‚îÄ workflows/         # Workflow definitions wired into the orchestrator
‚îî‚îÄ‚îÄ tests/                 # Pytest coverage for config, DB, workflows, prompts
```

---

## üì¶ Prerequisites

- Python 3.12+
- Virtual environment (`python -m venv .venv` then activate)
- Install dependencies with [uv](https://github.com/astral-sh/uv):

```bash
uv pip sync requirements.lock
```

- The lock file already captures both runtime and development dependencies (e.g., pytest).

- Set provider credentials (example `.env` entry):

```
AZURE_API_BASE="https://<your-endpoint>.openai.azure.com/"
AZURE_OPENAI_KEY="..."
OPENAI_API_KEY="..."
ANTHROPIC_API_KEY="..."
```

Export or load them in your shell before running the CLI. Providers and models are configured in `config/models.yaml` and `config/providers.yaml`.

---

## ‚öôÔ∏è Configuration

- `config/settings.yaml` ‚Äî app metadata & logging defaults.
- `config/database.yaml` ‚Äî SQLite path and Chroma persistence directory.
- `config/models.yaml` ‚Äî workflow ‚Üí model mappings, embedding model, provider defaults.
- `config/providers.yaml` ‚Äî provider endpoints and env var names for API keys.

All files are loaded through `ConfigService` and cached for reuse.

---

## ‚ñ∂Ô∏è Running the CLI

```bash
python -m src.cli describe "I can't decide between two great job offers."
python -m src.cli analyze
python -m src.cli explain
python -m src.cli simulate --scenario "Negotiating counter-offers"
python -m src.cli compare --limit 3
python -m src.cli settings show
python -m src.cli settings update-workflow detect_technique --model openai/gpt-4.1 --temperature 0.4
python -m src.cli refresh --skip-embeddings
python -m src.cli analyze --show-candidates
python -m src.cli feedback "Loved the recommendation" --rating 5 --technique "Decisional Balance"
python -m src.cli analyze --log-level DEBUG  # temporary verbose logging
```

Notes:
- `describe` saves the problem description and persists it to `data/state.json`.
- `analyze` runs the `detect_technique` workflow (LLM + vector search).
- `explain` requests the `explain_logic` workflow to justify the recommendation.
- `settings show` prints a JSON snapshot of the current config values. Use `update-workflow` and `update-provider` to make inline edits (supports `--interactive`).
- `feedback` stores feedback and summarizes recent entries via LLM.
- `simulate` replays the current recommendation with scenario variations and safeguards.
- `compare` analyzes the candidate shortlist, highlighting trade-offs and flow-on guidance.
- `refresh` reloads `data/techniques.json`, replaces existing rows, and optionally rebuilds embeddings.
- `analyze --show-candidates` prints the shortlist with similarity scores for transparency.

If the LLM provider rejects a parameter (e.g., unsupported temperature), adjust `config/models.yaml` or set `litellm.drop_params = True` before running the CLI.

---

## üß† Knowledge Base & Embeddings

1. Seed techniques in `data/techniques.json`.
2. On startup, `TechniqueDataInitializer` populates SQLite (if empty) and attempts to embed each technique using the configured embedding model. When Chroma is available, embeddings are stored and used for semantic retrieval.
3. If embedding APIs are unreachable, a deterministic hash-based fallback keeps the system functional (with reduced semantic quality).

To extend the library:
- Add rows to `data/techniques.json`.
- Run `python -m src.cli refresh` to reload the dataset (use `--skip-embeddings` when you only need SQLite updates).

---

## üß™ Testing

After syncing dependencies with `uv pip sync requirements.lock`, run:

```bash
pytest
```

Coverage includes config loading, SQLite operations, prompt registry validation, data initialization, and orchestrator execution.

---

## üîÑ Development Notes

- Use the `apply_patch` helper (if working in the Codex CLI) for targeted edits.
- Avoid committing provider secrets. `.env` is listed for guidance only.
- The project favors clear, traceable service layers; keep additions modular and register new workflows with the orchestrator.
- For future enhancements consider:
  - Additional workflows (e.g., scenario simulation).
  - Extended prompts and richer feedback analysis.

---

## üìÑ License

Project licensing has not been specified. Add a `LICENSE` file if you plan to open-source the repository.

---

Happy mapping! üéØ‚ú®
